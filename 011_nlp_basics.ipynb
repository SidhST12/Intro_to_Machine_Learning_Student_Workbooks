{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic NLP\n",
    "\n",
    "## Natural Language Processing - NLP\n",
    "\n",
    "NLP is processing natual language - free text and speech. We can use free text in predictive modelling, in fact it is a quickly developing field.\n",
    "\n",
    "Technologies such as speech recognition, automatic translation, and computer speech are all based on the concpets that we'll cover here. \n",
    "\n",
    "The premise of NLP is that we take a piece of text and process it to transform into a format that we can process. In our case here we'll take free text and convert it into a set of features - we can then use those features to make predictions for our target, just like always!\n",
    "\n",
    "#### Example Exercise - Spam Filtering\n",
    "\n",
    "For an example we'll build a spam filter. The dataset here has two columns - one is a text message, the other is a human assigned label of spam or ham. We want to be able to detect the spam messages and filter them out. The only feature we have to be able to do so is the message itself..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                               text\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Data\n",
    "df = pd.read_csv(\"data/spam.csv\", encoding=\"ISO-8859-1\")\n",
    "df.drop(columns={\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"}, inplace=True)\n",
    "df.rename(columns={\"v1\":\"target\", \"v2\":\"text\"}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Construction\n",
    "\n",
    "Unlike the data that we are used to, our free text doesn't really have a set of features, only one feature that contains random length snipits of text. Feeding in random text messages to a predictive algorithm is unlikely to be effective. The first step in using natural text as an input for our predictive models is to transform our data into a usable feature set that we can feed into a model. This transformation will result in our 1 dimension (kind of) free text turning into a (probably) very high dimension set of features. \n",
    "\n",
    "### Tokenization\n",
    "\n",
    "The first step in transforming the data is to extract each word from the original text - this process is called Tokenizing. Tokenizing takes a sentance and transforms it into a list of tolkens - the words in the sentence. \n",
    "\n",
    "![Tokenization](images/tokenization.png \"Tokenization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Very Simple Tokenizer\n",
    "\n",
    " We can visualize the process of tokenization pretty easily by looking at an example of a dead simple tokenizer. The function below will tokenize a sentence in a basic way - it will chop apart the sentence into words, and add them to a list. This example uses regex to do basic filtering to only extract words that are 2+ letters. \n",
    "\n",
    "<b>Note:</b> This example of a tokenizer (and this stuff in general) is a very basic version, and the field of NLP is developing quickly. More advanced text processing is better able to capture more of the structure of the language, and more of the meaning. We are stripping lots of \"hidden\" meaning out to make it manageable, more advanced NLP tries to understand as much of that meaning as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Really simple tokenizer\n",
    "def tokenize(sentence):\n",
    "    tokens = []\n",
    "    for token in re.findall(r\"\\b\\w\\w+\\b\", sentence):\n",
    "        tokens.append(token.lower())\n",
    "    return tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize a Thing...\n",
    "\n",
    "We can look at the example of one of our sentences being transformed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, å£1.50 to rcv\n",
      "['freemsg', 'hey', 'there', 'darling', 'it', 'been', 'week', 'now', 'and', 'no', 'word', 'back', 'like', 'some', 'fun', 'you', 'up', 'for', 'it', 'still', 'tb', 'ok', 'xxx', 'std', 'chgs', 'to', 'send', '50', 'to', 'rcv']\n"
     ]
    }
   ],
   "source": [
    "tolk = tokenize(df[\"text\"][5])\n",
    "print(df[\"text\"][5])\n",
    "print(tolk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize Results\n",
    "\n",
    "Tokenizing transforms our random text into something more orderly and able to be processed - in this case a list of words. This tokenization process is the basis of all other processing. \n",
    "\n",
    "We can take this set of tokens and do some further processing. For this we'll use something called a Vectorizer. The vectorizer will do the simple act of tokenizing, and build the actual data structure that we need as a feature set. \n",
    "\n",
    "#### Vocabulary\n",
    "\n",
    "The set of all our tokens, or all words used in our dataset is called the vocabulary. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizers\n",
    "\n",
    "In sklearn we have some libraries called vectorizors - they can do much of the text processing for us. There are two that we'll touch on - CountVectorizer and Tf-idf Vectorizer. \n",
    "\n",
    "Each of these does the bulk of the prep for us:\n",
    "<ul>\n",
    "<li> Tokenize the strings. \n",
    "<li> Count the occurances of each. \n",
    "<li> Weight the relative importance of different words. In different ways...\n",
    "<li> Produce a usable feature set. \n",
    "</ul> \n",
    "\n",
    "<b> Each takes in a dataset of text strings and outputs a set of features that we can use for our predictions. </b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorization\n",
    "\n",
    "Count vectorization is the most simple process we can use to make our text into a set of features. The count vectorization will split apart our data into tokens, count them up, and produce an array where:\n",
    "<ul>\n",
    "<li> Each column is a word. \n",
    "<li> Each row is an input piece of text (e.g. an email)\n",
    "<li> Each cell is a count of the number of times that word appears. \n",
    "</ul>\n",
    "\n",
    "![Count Vectorization](images/count_vector.png \"Count Vectorization\")\n",
    "\n",
    "This is our Bag of Words - now instead of having a sentence as an input, we have something like a one-hot matrix of words used. We can picture this by printing it out (Note: there's a little reconstruction below to put it into a nice dataframe format)\n",
    "\n",
    "#### Count Vectorizer Benefits and Drawbacks\n",
    "\n",
    "The main benefit of the count vectorizer is the simplicity and speed - all it needs to do is count. It has the downside of being quite simple in the analysis of the language - we don't extract which words are more or less important, we just get a count. For things that are written similarly this can be effective - I have used this for a simple tool to detect cheaters on tests - people copying from each other or a common source like Chegg tend to have the same words repeated in their answer. \n",
    "\n",
    "#### Sparse Features\n",
    "\n",
    "This process generally produces a sparse matrix - most words are not in most sentences, so most scores in the final matrix are 0. For this we'll keep it simple and use algorithms that deal with sparse matrices (e.g. SVC). Later on we'll look at ways to reduce dimensionality.\n",
    "\n",
    "Some algorithms may throw an error if you feed them a sparse matrix. \n",
    "\n",
    "### Use Count Vectorizer\n",
    "\n",
    "We can look at the dataset that is generated for us after using the count vectorizer. The mechanics are very similar to the other sklearn transformers that we've used. The output is an array, so there's a little extra code there to put it into a dataframe for easy viewing. For the first try I'll set a limit of 150 features, so only the most common 150 tokens will be kept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: (5572,)\n",
      "vectorized: (5572, 150)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shikh\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>after</th>\n",
       "      <th>all</th>\n",
       "      <th>already</th>\n",
       "      <th>am</th>\n",
       "      <th>amp</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>any</th>\n",
       "      <th>are</th>\n",
       "      <th>...</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>won</th>\n",
       "      <th>work</th>\n",
       "      <th>www</th>\n",
       "      <th>yeah</th>\n",
       "      <th>yes</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "      <th>ì_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4119</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2654</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2646</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3820</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3654</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4923</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4009</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      about  after  all  already  am  amp  an  and  any  are  ...  will  with  \\\n",
       "4119      0      0    0        1   0    0   0    0    0    1  ...     0     0   \n",
       "109       0      0    0        0   0    0   0    0    0    0  ...     0     0   \n",
       "2654      0      0    0        0   0    0   0    0    0    0  ...     0     0   \n",
       "2646      1      0    0        0   0    0   0    0    0    0  ...     0     0   \n",
       "1282      0      0    0        0   0    0   0    0    0    0  ...     0     0   \n",
       "3820      0      1    0        0   0    0   1    2    0    2  ...     0     0   \n",
       "1346      0      0    0        0   0    0   0    1    0    0  ...     0     0   \n",
       "3654      0      0    0        0   0    0   0    0    0    0  ...     0     0   \n",
       "4923      0      1    0        0   0    0   0    0    0    0  ...     0     0   \n",
       "4009      0      0    0        0   0    0   0    0    0    1  ...     0     1   \n",
       "\n",
       "      won  work  www  yeah  yes  you  your  ì_  \n",
       "4119    0     0    0     0    0    2     0   0  \n",
       "109     0     0    0     0    0    0     0   0  \n",
       "2654    0     0    0     0    0    0     0   0  \n",
       "2646    0     0    0     0    0    0     0   0  \n",
       "1282    0     0    0     0    0    0     0   0  \n",
       "3820    0     0    0     0    0    4     0   0  \n",
       "1346    0     0    0     0    0    0     0   0  \n",
       "3654    0     0    0     0    0    0     0   0  \n",
       "4923    0     0    0     0    0    0     0   0  \n",
       "4009    0     0    0     0    0    0     0   0  \n",
       "\n",
       "[10 rows x 150 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_cv = CountVectorizer(max_features=150)\n",
    "tmp = vec_cv.fit_transform(df[\"text\"])\n",
    "tok_cols = vec_cv.get_feature_names()\n",
    "tok_df = pd.DataFrame(tmp.toarray(), columns=tok_cols)\n",
    "print(\"original:\", df[\"text\"].shape)\n",
    "print(\"vectorized:\", tmp.shape)\n",
    "tok_df.sample(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That has the number of features limited to 150, if we imposed no limits we'd get something way messier... If we look at some of the words that we can see in the columns below, we can surmise that we are probably getting a bunch of junk that isn't all that useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: (5572,)\n",
      "vectorized: (5572, 8672)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shikh\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>...</th>\n",
       "      <th>ó_</th>\n",
       "      <th>û_</th>\n",
       "      <th>û_thanks</th>\n",
       "      <th>ûªm</th>\n",
       "      <th>ûªt</th>\n",
       "      <th>ûªve</th>\n",
       "      <th>ûï</th>\n",
       "      <th>ûïharry</th>\n",
       "      <th>ûò</th>\n",
       "      <th>ûówell</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1945</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2779</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3731</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4812</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4663</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3389</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 8672 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00  000  000pes  008704050406  0089  0121  01223585236  01223585334  \\\n",
       "1945   0    0       0             0     0     0            0            0   \n",
       "2439   0    0       0             0     0     0            0            0   \n",
       "2779   0    0       0             0     0     0            0            0   \n",
       "48     0    0       0             0     0     0            0            0   \n",
       "3731   0    0       0             0     0     0            0            0   \n",
       "4812   0    0       0             0     0     0            0            0   \n",
       "4663   0    0       0             0     0     0            0            0   \n",
       "3099   0    0       0             0     0     0            0            0   \n",
       "698    0    0       0             0     0     0            0            0   \n",
       "3389   0    0       0             0     0     0            0            0   \n",
       "\n",
       "      0125698789  02  ...  ó_  û_  û_thanks  ûªm  ûªt  ûªve  ûï  ûïharry  ûò  \\\n",
       "1945           0   0  ...   0   0         0    0    0     0   0        0   0   \n",
       "2439           0   0  ...   0   0         0    0    0     0   0        0   0   \n",
       "2779           0   0  ...   0   0         0    0    0     0   0        0   0   \n",
       "48             0   0  ...   0   0         0    0    0     0   0        0   0   \n",
       "3731           0   0  ...   0   0         0    0    0     0   0        0   0   \n",
       "4812           0   0  ...   0   0         0    0    0     0   0        0   0   \n",
       "4663           0   0  ...   0   0         0    0    0     0   0        0   0   \n",
       "3099           0   0  ...   0   0         0    0    0     0   0        0   0   \n",
       "698            0   0  ...   0   0         0    0    0     0   0        0   0   \n",
       "3389           0   0  ...   0   0         0    0    0     0   0        0   0   \n",
       "\n",
       "      ûówell  \n",
       "1945       0  \n",
       "2439       0  \n",
       "2779       0  \n",
       "48         0  \n",
       "3731       0  \n",
       "4812       0  \n",
       "4663       0  \n",
       "3099       0  \n",
       "698        0  \n",
       "3389       0  \n",
       "\n",
       "[10 rows x 8672 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_cv2 = CountVectorizer()\n",
    "tmp2 = vec_cv2.fit_transform(df[\"text\"])\n",
    "tok_cols2 = vec_cv2.get_feature_names()\n",
    "tok_df2 = pd.DataFrame(tmp2.toarray(), columns=tok_cols2)\n",
    "print(\"original:\", df[\"text\"].shape)\n",
    "print(\"vectorized:\", tmp2.shape)\n",
    "tok_df2.sample(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count Vectorizer Results\n",
    "\n",
    "Each of the vectorizations above delivered us a fully formed feature set. Each row is one piece of our input text, each column is a word, and each cell is the count of the number of times that word occurs in that text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization\n",
    "\n",
    "TF-IDF vectorization is similar to the count vectorizor, but it does some calculations to determine the importance of the word. The calculations are based on the name:\n",
    "<ul>\n",
    "<li> <b>Term Frequency</b> - the number of times a word appears in a document divided by the total number of words in the document.\n",
    "<li> <b>Inverse Document Frequency</b> - the log of the total number of documents divided by the number of documents that contain the word.\n",
    "<li> <b>Note:</b> each input phrase (row in dataset) is a document. \n",
    "</ul>\n",
    "\n",
    "![TF-IDF](images/tfidf.png \"TF-IDF\" )\n",
    "\n",
    "The final result is the two multiplied by each other, hence TF-IDF. \n",
    "\n",
    "#### TF-IDF Importance\n",
    "\n",
    "TF-IDF weights the importance of each word to give lower scores to words that are:\n",
    "<ul>\n",
    "<li> <b>Too frequent</b> - words that repeat constantly are likely to not be helpful in differentiating sentences. \n",
    "    <ul>\n",
    "    <li> \"the\", \"it\", \"and\", \"to\", \"for\", etc. and other common words occur in a huge proportion of documents, so they are not very useful in differentiating between documents.\n",
    "    <li> In specific applications, other words that are common in that domain may also become too frequent.\n",
    "    </ul>\n",
    "<li> <b>Too rare</b> - words that almost never occur don't exist often enough to establish a pattern. \n",
    "    <ul>\n",
    "    <li> If words only extremely occasionally in our dataset, those rare words are not likely to be useful in differentiating between documents, since we just don't see them enough to establish any sort of pattern.\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "TF-IDF tends to give us a better ability to evaluate work importance, but it is still not able to extract relationships between words nor generate more sophisticated meaning of the words. For that we need to use more sophisticated processing libraries, such as word2vec that we'll look at later on. \n",
    "\n",
    "<b>Note:</b> a small change here in the max features argument, now it'll keep the 150 overall highest scoring tokens. This is slightly differnet from the most frequent, as those words that score poorly in the td-idf calculation will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: (5572,)\n",
      "vectorized: (5572, 150)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shikh\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>after</th>\n",
       "      <th>all</th>\n",
       "      <th>already</th>\n",
       "      <th>am</th>\n",
       "      <th>amp</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>any</th>\n",
       "      <th>are</th>\n",
       "      <th>...</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>won</th>\n",
       "      <th>work</th>\n",
       "      <th>www</th>\n",
       "      <th>yeah</th>\n",
       "      <th>yes</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "      <th>ì_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.357430</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2683</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.306532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.195933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3943</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1838</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5511</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.351081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.224859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.576468</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2410</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.621531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         about  after  all  already   am  amp   an       and  any       are  \\\n",
       "1775  0.000000    0.0  0.0      0.0  0.0  0.0  0.0  0.000000  0.0  0.000000   \n",
       "2683  0.000000    0.0  0.0      0.0  0.0  0.0  0.0  0.000000  0.0  0.306532   \n",
       "948   0.000000    0.0  0.0      0.0  0.0  0.0  0.0  0.000000  0.0  0.000000   \n",
       "3943  0.000000    0.0  0.0      0.0  0.0  0.0  0.0  0.000000  0.0  0.000000   \n",
       "1838  0.000000    0.0  0.0      0.0  0.0  0.0  0.0  0.000000  0.0  0.000000   \n",
       "5511  0.000000    0.0  0.0      0.0  0.0  0.0  0.0  0.000000  0.0  0.000000   \n",
       "10    0.351081    0.0  0.0      0.0  0.0  0.0  0.0  0.224859  0.0  0.000000   \n",
       "530   0.000000    0.0  0.0      0.0  0.0  0.0  0.0  0.000000  0.0  0.000000   \n",
       "2410  0.000000    0.0  0.0      0.0  0.0  0.0  0.0  0.000000  0.0  0.000000   \n",
       "21    0.000000    0.0  0.0      0.0  0.0  0.0  0.0  0.000000  0.0  0.000000   \n",
       "\n",
       "      ...  will  with  won  work  www  yeah  yes       you      your        ì_  \n",
       "1775  ...   0.0   0.0  0.0   0.0  0.0   0.0  0.0  0.000000  0.357430  0.000000  \n",
       "2683  ...   0.0   0.0  0.0   0.0  0.0   0.0  0.0  0.195933  0.000000  0.000000  \n",
       "948   ...   0.0   0.0  0.0   0.0  0.0   0.0  0.0  0.000000  0.000000  0.000000  \n",
       "3943  ...   0.0   0.0  0.0   0.0  0.0   0.0  0.0  0.000000  0.000000  0.000000  \n",
       "1838  ...   0.0   0.0  0.0   0.0  0.0   0.0  0.0  0.000000  0.000000  0.000000  \n",
       "5511  ...   0.0   0.0  0.0   0.0  0.0   0.0  0.0  0.000000  0.000000  0.000000  \n",
       "10    ...   0.0   0.0  0.0   0.0  0.0   0.0  0.0  0.000000  0.000000  0.000000  \n",
       "530   ...   0.0   0.0  0.0   0.0  0.0   0.0  0.0  0.000000  0.576468  0.000000  \n",
       "2410  ...   0.0   0.0  0.0   0.0  0.0   0.0  0.0  0.000000  0.000000  0.621531  \n",
       "21    ...   0.0   0.0  0.0   0.0  0.0   0.0  0.0  0.000000  0.000000  0.000000  \n",
       "\n",
       "[10 rows x 150 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDF\n",
    "vec_tf = TfidfVectorizer(max_features=150)\n",
    "tmp = vec_tf.fit_transform(df[\"text\"])\n",
    "tok_cols = vec_tf.get_feature_names()\n",
    "tok_df = pd.DataFrame(tmp.toarray(), columns=tok_cols)\n",
    "print(\"original:\", df[\"text\"].shape)\n",
    "print(\"vectorized:\", tmp.shape)\n",
    "tok_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization Parameters\n",
    "\n",
    "There are several parameters that can be pretty important when doing vectorization:\n",
    "<ul>\n",
    "<li> Max Features - as seen above. Limits how many feature columns are produced. This will cap it to the N most frequent words instead of every word seen. \n",
    "<li> strip_accents - remove random characters such as accents. \n",
    "<li> lowercase - covert all to lower case. This is helpful as case matters in code, but doesn't matter for us. \n",
    "<li> stop_words - filter out stop words. More on this later. \n",
    "<li> tokenizer - we can specify our own tokenizer function, where we can layer in more processing. More on this later. \n",
    "<li> ngram_range - how \"big\" can tokens be? I.e. can you have a 2 word token - e.g. \"downhill skiing\". \n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-Grams\n",
    "\n",
    "N-grams are a way to capture the relationship between words. For example, the phrase \"downhill skiing\" is a 2-gram, those two words together are a specific \"thing\", different from the words \"downhill\" and \"skiing\" by themselves. The N in n-grams is the number of words in the phrase that are stored as one token. Allowing n-grams that are longer than one word cam be extremely helpful in allowing our feature set to better capture the meaning of our text, it is common to have multi-word terms-of-art, product names, descriptions (e.g. \"dirty blonde\"), and so on. On the (potential) downside, it can increase the number of features dramatically, and can make the feature set more sparse. When allowing larger n-grams, particularly, it is common to limit the number of features up front and/or use some techniques later on to reduce the dimensionality of the feature set. A feature set that is 20 times wider than it is tall is probably not going to be ideal. \n",
    "\n",
    "![N-Gram Vectorization](images/ngram.png \"N-Gram Vectorization\" )\n",
    "\n",
    "##### N-Gram Vectorization\n",
    "\n",
    "We can allow longer n-grams with a hyperparameter, for this example I allowed things up to 3, we can see that the number of features will explode in size as now any up to n-word long sequence can be a feature in our feature set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shikh\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: (5572,)\n",
      "vectorized: (5572, 150)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>00 in</th>\n",
       "      <th>00 in our</th>\n",
       "      <th>00 per</th>\n",
       "      <th>00 sub</th>\n",
       "      <th>00 sub 16</th>\n",
       "      <th>00 subs</th>\n",
       "      <th>00 subs 16</th>\n",
       "      <th>000</th>\n",
       "      <th>000 bonus</th>\n",
       "      <th>...</th>\n",
       "      <th>ûò is limping</th>\n",
       "      <th>ûò sound</th>\n",
       "      <th>ûò sound ok</th>\n",
       "      <th>ûò to</th>\n",
       "      <th>ûò to an</th>\n",
       "      <th>ûò very</th>\n",
       "      <th>ûò very entertaining</th>\n",
       "      <th>ûówell</th>\n",
       "      <th>ûówell done</th>\n",
       "      <th>ûówell done û_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3233</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2895</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4813</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5286</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2046</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 104564 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       00  00 in  00 in our  00 per  00 sub  00 sub 16  00 subs  00 subs 16  \\\n",
       "3233  0.0    0.0        0.0     0.0     0.0        0.0      0.0         0.0   \n",
       "281   0.0    0.0        0.0     0.0     0.0        0.0      0.0         0.0   \n",
       "2895  0.0    0.0        0.0     0.0     0.0        0.0      0.0         0.0   \n",
       "70    0.0    0.0        0.0     0.0     0.0        0.0      0.0         0.0   \n",
       "1001  0.0    0.0        0.0     0.0     0.0        0.0      0.0         0.0   \n",
       "4813  0.0    0.0        0.0     0.0     0.0        0.0      0.0         0.0   \n",
       "5286  0.0    0.0        0.0     0.0     0.0        0.0      0.0         0.0   \n",
       "1091  0.0    0.0        0.0     0.0     0.0        0.0      0.0         0.0   \n",
       "1849  0.0    0.0        0.0     0.0     0.0        0.0      0.0         0.0   \n",
       "2046  0.0    0.0        0.0     0.0     0.0        0.0      0.0         0.0   \n",
       "\n",
       "      000  000 bonus  ...  ûò is limping  ûò sound  ûò sound ok  ûò to  \\\n",
       "3233  0.0        0.0  ...            0.0       0.0          0.0    0.0   \n",
       "281   0.0        0.0  ...            0.0       0.0          0.0    0.0   \n",
       "2895  0.0        0.0  ...            0.0       0.0          0.0    0.0   \n",
       "70    0.0        0.0  ...            0.0       0.0          0.0    0.0   \n",
       "1001  0.0        0.0  ...            0.0       0.0          0.0    0.0   \n",
       "4813  0.0        0.0  ...            0.0       0.0          0.0    0.0   \n",
       "5286  0.0        0.0  ...            0.0       0.0          0.0    0.0   \n",
       "1091  0.0        0.0  ...            0.0       0.0          0.0    0.0   \n",
       "1849  0.0        0.0  ...            0.0       0.0          0.0    0.0   \n",
       "2046  0.0        0.0  ...            0.0       0.0          0.0    0.0   \n",
       "\n",
       "      ûò to an  ûò very  ûò very entertaining  ûówell  ûówell done  \\\n",
       "3233       0.0      0.0                   0.0     0.0          0.0   \n",
       "281        0.0      0.0                   0.0     0.0          0.0   \n",
       "2895       0.0      0.0                   0.0     0.0          0.0   \n",
       "70         0.0      0.0                   0.0     0.0          0.0   \n",
       "1001       0.0      0.0                   0.0     0.0          0.0   \n",
       "4813       0.0      0.0                   0.0     0.0          0.0   \n",
       "5286       0.0      0.0                   0.0     0.0          0.0   \n",
       "1091       0.0      0.0                   0.0     0.0          0.0   \n",
       "1849       0.0      0.0                   0.0     0.0          0.0   \n",
       "2046       0.0      0.0                   0.0     0.0          0.0   \n",
       "\n",
       "      ûówell done û_  \n",
       "3233             0.0  \n",
       "281              0.0  \n",
       "2895             0.0  \n",
       "70               0.0  \n",
       "1001             0.0  \n",
       "4813             0.0  \n",
       "5286             0.0  \n",
       "1091             0.0  \n",
       "1849             0.0  \n",
       "2046             0.0  \n",
       "\n",
       "[10 rows x 104564 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDF\n",
    "vec_tf_ng = TfidfVectorizer(ngram_range=[1,3])\n",
    "tmp_ng = vec_tf_ng.fit_transform(df[\"text\"])\n",
    "tok_cols_ng = vec_tf_ng.get_feature_names()\n",
    "tok_df_ng = pd.DataFrame(tmp_ng.toarray(), columns=tok_cols_ng)\n",
    "print(\"original:\", df[\"text\"].shape)\n",
    "print(\"vectorized:\", tmp.shape)\n",
    "tok_df_ng.sample(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Model with Text\n",
    "\n",
    "We now have enough tools to use our text as a feature set, and train a predictive model. We can probably be a bit smarter with how we process our text, but for a first pass, we have something that works. Try running the code below with each vectorizer. \n",
    "\n",
    "Support vector machines are a good choice for text classification, as they are able to handle sparse feature sets without adaptation, and the model tends to deliver accurate predictions for this type of problem. The sparse feature set thing is a small concern for now, we'll look at ways to reduce the dimensionality of the feature set later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      1.00      0.99      1196\n",
      "        spam       0.98      0.84      0.90       197\n",
      "\n",
      "    accuracy                           0.97      1393\n",
      "   macro avg       0.98      0.92      0.94      1393\n",
      "weighted avg       0.98      0.97      0.97      1393\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model_svc = SVC()\n",
    "\n",
    "vec_cv = CountVectorizer(max_features=150, ngram_range=[1,2])\n",
    "\n",
    "y = df[\"target\"]\n",
    "X = df[\"text\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "pipe1 = Pipeline([ \n",
    "                    (\"vect\", vec_cv),\n",
    "                    (\"model\", model_svc)\n",
    "])\n",
    "\n",
    "params = [\"vec_cv\"]\n",
    "\n",
    "pipe1.fit(X_train, y_train.ravel())\n",
    "preds = pipe1.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds))\n",
    "sns.heatmap(confusion_matrix(y_test, preds), annot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "Our predictions are pretty good!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Try with TD-IDF\n",
    "\n",
    "Try the previous prediction with the td-idf vectorizer. Play around with the ngrams if you have time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Elaborate Language Processing\n",
    "\n",
    "In the example above we've done a \"base\" level of modelling - we transform the free text into something that we can process (the bag of words), and we can make predictions from it much like we would with any other one-hot encoded data. This process works fine, and it does deliver some pretty accurate results on our test data. \n",
    "\n",
    "To create NLP models that are more functional we can add some layers to our processing of the text to improve our understanding of the nuances of our text. Some things we can do are:\n",
    "<ul>\n",
    "<li> <b>Remove Stop Words</b> - common words like \"it\", \"a\", \"the\" are normally not all that useful in predicting the meaning, we can filter these out. \n",
    "<li> <b>Stemming</b> - coverting words down to their \"stem\". E.g. \"reasoning\" to \"reason\"\n",
    "<li> <b>Lemmatization</b> - similar to stemming, but tries to identify the correct stem contextually. E.g. \"Operating systems\" probably shouldn't become \"operate\" and \"system\"\n",
    "</ul>\n",
    "\n",
    "In general, stemming increases recall while harming precision. Lemmatization has similar impacts, but tends to be less aggressive, so the effects are smaller. The specific results are highly variable depending on the exact text that is used. Something that uses specific variations of words to mean specific things (e.g. science) is more likely to get no benefit or be negatively impacted - e.g. \"conditonally\" used in the context of a \"conditionally approved loan\" is probably not well represented by changing it to \"condition\". \n",
    "\n",
    "### NLTK Library\n",
    "\n",
    "NLTK is a library that provides a bunch of language processing stuff that we can use such as stop words and tokenizers. We'll leverage it here to make custom tokenizers to incorporate some of those features above. The things we are downloading here are pre-made sets of data, like stop words, and pretrained lists of \"root words\" that we can use to break words down into their root format. \n",
    "\n",
    "<b>Note:</b> the \"for package\" part there downloads the wordsets to your computer. NLTK has these prebuilt libraries of data that allow for the functions to do the stop words, stemming, and lemmatization. It might take a minute the first time you run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shikh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shikh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shikh\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "for package in ['stopwords','punkt','wordnet']:\n",
    "    nltk.download(package)\n",
    "    \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized Tokenizers\n",
    "\n",
    "The vecorization libraries in sklearn allow you do specify the function to use to do tokenization. We can use this to include other processing that we'd like as part of the process, such as removing stop words or stemming. The tokenizer functions below can, potentially, contain anything you'd like. As long as the call function returns a list of tokens, it should work. \n",
    "\n",
    "<b>Note:</b> if you look up examples, these functions will often be written into one lines, I broke them out so they're hopefully easier to read. They would also likely be much faster if we were to vectorize the code instead of using loops, but again, this is easy to read. \n",
    "\n",
    "#### Stop! In the Name of Words. Before you Break my Model.\n",
    "\n",
    "First, we will try to make a stop word tokenizer. If something is a stop word, we shall leave it out. As noted above, we can build this into the vectorizer, so why do it? This will allow for customizing the stopwords used - some applications may have a different usage of words, so changing stopwords makes sense. It is also a super fun exercise! \n",
    "\n",
    "This is also the most simple example we can try :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class swTokenizer(object):\n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "    def __call__(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        filtered_tok = []\n",
    "        for tok in tokens:\n",
    "            if tok not in stop_words:\n",
    "                filtered_tok.append(tok)\n",
    "        return filtered_tok"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "\n",
    "Stemming and lemmatization are similar - they both aim to break words down to their \"root\". For example, the word \"shoes\" probably has the same meaning as the word \"shoe\" for our purposes. Each approaches this in a slightly different way, and to understand that we need to take a look at the conecpt of similarity, which we'll look at more next time. \n",
    "\n",
    "#### Similarity\n",
    "\n",
    "When processing text, we can think of things being similar in two different ways - similar text or similar meaning - or lexical and semantic similarity. Things that are lexically similar use similar words, things that are semantically similar have similar meanings, even if the words are different. The stemming techniques here aren't explicit comparisons of those similarity types, but they follow the same concepts. Stemming breaks words down to their lexical root, lemmatization tries to find the semantic root.\n",
    "\n",
    "#### Stemming\n",
    "\n",
    "Stemming is the most simple, it just removes common prefixes and suffixes to extract the root of the word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stemTokenizer(object):\n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "        from nltk.stem import SnowballStemmer\n",
    "        self.stemmer = SnowballStemmer(language='english')\n",
    "    def __call__(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        filtered_tok = []\n",
    "        for tok in tokens:\n",
    "            if tok not in stop_words:\n",
    "                filtered_tok.append(self.stemmer.stem(tok))\n",
    "        return filtered_tok"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "Lemmatization is slightly more sophisticated, it attempts to find the semantic root, called the lemma, of a word using a search of a dictionary (we provide one from NLTK). For example, the lemma of \"better\" is \"good\". This is a \"smarter\" approach that the more simple stemming function above, but it is also more complex and slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lemmaTokenizer(object):\n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        filtered_tok = []\n",
    "        for tok in tokens:\n",
    "            if tok not in stop_words:\n",
    "                filtered_tok.append(self.lemmatizer.lemmatize(tok))\n",
    "        return filtered_tok"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions with More Processing and Grid Search\n",
    "\n",
    "We can try to see which processing setup works best, the winner will depend on how the text is written. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shikh\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "40 fits failed out of a total of 120.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "40 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shikh\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\", line 84, in __load\n",
      "    root = nltk.data.find(f\"{self.subdir}/{zip_name}\")\n",
      "  File \"c:\\Users\\shikh\\anaconda3\\lib\\site-packages\\nltk\\data.py\", line 583, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93momw-1.4\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('omw-1.4')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mcorpora/omw-1.4.zip/omw-1.4/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\shikh/nltk_data'\n",
      "    - 'c:\\\\Users\\\\shikh\\\\anaconda3\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\shikh\\\\anaconda3\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\shikh\\\\anaconda3\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\shikh\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shikh\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shikh\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 378, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\Users\\shikh\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 336, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\Users\\shikh\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\Users\\shikh\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"c:\\Users\\shikh\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 2079, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"c:\\Users\\shikh\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1338, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "  File \"c:\\Users\\shikh\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1209, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"c:\\Users\\shikh\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 113, in _analyze\n",
      "    doc = tokenizer(doc)\n",
      "  File \"C:\\Users\\shikh\\AppData\\Local\\Temp\\ipykernel_44764\\1444532330.py\", line 11, in __call__\n",
      "  File \"c:\\Users\\shikh\\anaconda3\\lib\\site-packages\\nltk\\stem\\wordnet.py\", line 45, in lemmatize\n",
      "    lemmas = wn._morphy(word, pos)\n",
      "  File \"c:\\Users\\shikh\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\", line 121, in __getattr__\n",
      "    self.__load()\n",
      "  File \"c:\\Users\\shikh\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\", line 89, in __load\n",
      "    corpus = self.__reader_cls(root, *self.__args, **self.__kwargs)\n",
      "  File \"c:\\Users\\shikh\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\", line 1176, in __init__\n",
      "    self.provenances = self.omw_prov()\n",
      "  File \"c:\\Users\\shikh\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\", line 1285, in omw_prov\n",
      "    fileids = self._omw_reader.fileids()\n",
      "  File \"c:\\Users\\shikh\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\", line 121, in __getattr__\n",
      "    self.__load()\n",
      "  File \"c:\\Users\\shikh\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\", line 86, in __load\n",
      "    raise e\n",
      "  File \"c:\\Users\\shikh\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\", line 81, in __load\n",
      "    root = nltk.data.find(f\"{self.subdir}/{self.__name}\")\n",
      "  File \"c:\\Users\\shikh\\anaconda3\\lib\\site-packages\\nltk\\data.py\", line 583, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93momw-1.4\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('omw-1.4')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\shikh/nltk_data'\n",
      "    - 'c:\\\\Users\\\\shikh\\\\anaconda3\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\shikh\\\\anaconda3\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\shikh\\\\anaconda3\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\shikh\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\shikh\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.88970376 0.89178093        nan 0.89653644 0.89741324        nan\n",
      " 0.9210988  0.93305633        nan 0.91772853 0.91832872        nan\n",
      " 0.93291782 0.93845799        nan 0.9232687  0.9232687         nan\n",
      " 0.93042475 0.93758079        nan 0.92253001 0.92063712        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('vect',\n",
      "                 TfidfVectorizer(max_features=1000, norm='l1',\n",
      "                                 tokenizer=<__main__.stemTokenizer object at 0x00000204CB212AF0>)),\n",
      "                ('model', SVC())])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1216\n",
      "        spam       0.99      0.89      0.94       177\n",
      "\n",
      "    accuracy                           0.99      1393\n",
      "   macro avg       0.99      0.95      0.97      1393\n",
      "weighted avg       0.99      0.99      0.99      1393\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vec_tf = TfidfVectorizer()\n",
    "\n",
    "y = df[\"target\"]\n",
    "X = df[\"text\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "pipe2 = Pipeline([ \n",
    "                    #(\"vect\", vec_cv),\n",
    "                    (\"vect\", vec_tf),\n",
    "                    (\"model\", model_svc)\n",
    "])\n",
    "\n",
    "params = {\"vect__max_features\":[100,500,1000,1500],\n",
    "            \"vect__tokenizer\":(swTokenizer(stop_words), stemTokenizer(stop_words), lemmaTokenizer(stop_words) ),\n",
    "            \"vect__norm\":[\"l1\",\"l2\"]\n",
    "            }\n",
    "\n",
    "grid = GridSearchCV(estimator  = pipe2, \n",
    "                               param_grid = params, \n",
    "                               scoring    = \"balanced_accuracy\",\n",
    "                               cv         = 5,\n",
    "                               n_jobs     =-1)\n",
    "\n",
    "grid.fit(X_train, y_train.ravel())\n",
    "best = grid.best_estimator_\n",
    "preds = best.predict(X_test)\n",
    "print(best)\n",
    "print(classification_report(y_test, preds))\n",
    "sns.heatmap(confusion_matrix(y_test, preds), annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results\n",
    "\n",
    "We are accurate! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Categorize the following newsgroups. The data are posts from different newgroup boards. Try to categorize the data in either the atheism or religion groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\shikh\\OneDrive\\Documents\\DATA-3950\\Intro_to_Machine_Learning_Student_Workbooks\\011_nlp_basics.ipynb Cell 38\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shikh/OneDrive/Documents/DATA-3950/Intro_to_Machine_Learning_Student_Workbooks/011_nlp_basics.ipynb#X52sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m remove \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfooters\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mquotes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shikh/OneDrive/Documents/DATA-3950/Intro_to_Machine_Learning_Student_Workbooks/011_nlp_basics.ipynb#X52sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m categories \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39malt.atheism\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtalk.religion.misc\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/shikh/OneDrive/Documents/DATA-3950/Intro_to_Machine_Learning_Student_Workbooks/011_nlp_basics.ipynb#X52sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m data_train \u001b[39m=\u001b[39m fetch_20newsgroups(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shikh/OneDrive/Documents/DATA-3950/Intro_to_Machine_Learning_Student_Workbooks/011_nlp_basics.ipynb#X52sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     subset\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m, categories\u001b[39m=\u001b[39;49mcategories, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m, remove\u001b[39m=\u001b[39;49mremove)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shikh/OneDrive/Documents/DATA-3950/Intro_to_Machine_Learning_Student_Workbooks/011_nlp_basics.ipynb#X52sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m data_test \u001b[39m=\u001b[39m fetch_20newsgroups(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shikh/OneDrive/Documents/DATA-3950/Intro_to_Machine_Learning_Student_Workbooks/011_nlp_basics.ipynb#X52sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     subset\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, categories\u001b[39m=\u001b[39mcategories, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, remove\u001b[39m=\u001b[39mremove)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shikh/OneDrive/Documents/DATA-3950/Intro_to_Machine_Learning_Student_Workbooks/011_nlp_basics.ipynb#X52sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m X_train3 \u001b[39m=\u001b[39m data_train\u001b[39m.\u001b[39mdata\n",
      "File \u001b[1;32mc:\\Users\\shikh\\anaconda3\\lib\\site-packages\\sklearn\\datasets\\_twenty_newsgroups.py:269\u001b[0m, in \u001b[0;36mfetch_20newsgroups\u001b[1;34m(data_home, subset, categories, shuffle, random_state, remove, download_if_missing, return_X_y)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[39mif\u001b[39;00m download_if_missing:\n\u001b[0;32m    268\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mDownloading 20news dataset. This may take a few minutes.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 269\u001b[0m     cache \u001b[39m=\u001b[39m _download_20newsgroups(\n\u001b[0;32m    270\u001b[0m         target_dir\u001b[39m=\u001b[39;49mtwenty_home, cache_path\u001b[39m=\u001b[39;49mcache_path\n\u001b[0;32m    271\u001b[0m     )\n\u001b[0;32m    272\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m20Newsgroups dataset not found\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\shikh\\anaconda3\\lib\\site-packages\\sklearn\\datasets\\_twenty_newsgroups.py:89\u001b[0m, in \u001b[0;36m_download_20newsgroups\u001b[1;34m(target_dir, cache_path)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(cache_path, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     87\u001b[0m     f\u001b[39m.\u001b[39mwrite(compressed_content)\n\u001b[1;32m---> 89\u001b[0m shutil\u001b[39m.\u001b[39;49mrmtree(target_dir)\n\u001b[0;32m     90\u001b[0m \u001b[39mreturn\u001b[39;00m cache\n",
      "File \u001b[1;32mc:\\Users\\shikh\\anaconda3\\lib\\shutil.py:757\u001b[0m, in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[0;32m    755\u001b[0m     \u001b[39m# can't continue even if onerror hook returns\u001b[39;00m\n\u001b[0;32m    756\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m \u001b[39mreturn\u001b[39;00m _rmtree_unsafe(path, onerror)\n",
      "File \u001b[1;32mc:\\Users\\shikh\\anaconda3\\lib\\shutil.py:622\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    620\u001b[0m         onerror(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mislink, fullname, sys\u001b[39m.\u001b[39mexc_info())\n\u001b[0;32m    621\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m--> 622\u001b[0m     _rmtree_unsafe(fullname, onerror)\n\u001b[0;32m    623\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    624\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\shikh\\anaconda3\\lib\\shutil.py:622\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    620\u001b[0m         onerror(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mislink, fullname, sys\u001b[39m.\u001b[39mexc_info())\n\u001b[0;32m    621\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m--> 622\u001b[0m     _rmtree_unsafe(fullname, onerror)\n\u001b[0;32m    623\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    624\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\shikh\\anaconda3\\lib\\shutil.py:625\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    624\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 625\u001b[0m         os\u001b[39m.\u001b[39;49munlink(fullname)\n\u001b[0;32m    626\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m         onerror(os\u001b[39m.\u001b[39munlink, fullname, sys\u001b[39m.\u001b[39mexc_info())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "remove = (\"headers\", \"footers\", \"quotes\")\n",
    "categories = [\"alt.atheism\", \"talk.religion.misc\"]\n",
    "\n",
    "data_train = fetch_20newsgroups(\n",
    "    subset=\"train\", categories=categories, shuffle=True, random_state=42, remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(\n",
    "    subset=\"test\", categories=categories, shuffle=True, random_state=42, remove=remove)\n",
    "\n",
    "X_train3 = data_train.data\n",
    "y_train3 = data_train.target\n",
    "X_test3 = data_test.data\n",
    "y_test3 = data_test.target\n",
    "#X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac84c186c3977d3f3331649749bf53276a5c5befb029be0bc359ac378ed9e33b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
